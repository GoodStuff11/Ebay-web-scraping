{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from datetime import datetime\n",
    "from math import ceil\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sellerURL(url=None,bs=None):\n",
    "    #returns the url of a sellers items given the url of the seller\n",
    "    assert url or bs, 'no inputs'\n",
    "    if url and not bs:\n",
    "        html = requests.get(url + '&_pgn=1').text\n",
    "        bs = BeautifulSoup(html,'html.parser')\n",
    "    \n",
    "    new_url = bs.find('div',class_='si-pd-a').find('a',href=True)['href']\n",
    "    return new_url\n",
    "\n",
    "def getResults(url=None,bs=None):\n",
    "    #returns the number of results of an ebay screen given its url\n",
    "    assert url or bs, 'no inputs'\n",
    "    if url and not bs:\n",
    "        html = requests.get(url + '&_pgn=1').text\n",
    "        bs = BeautifulSoup(html,'html.parser')\n",
    "        \n",
    "    results = bs.body.find('span',class_='rcnt')\n",
    "    if results:\n",
    "        return int(results.string.replace(',',''))\n",
    "    \n",
    "    results = bs.body.find('h2',class_='srp-controls__count-heading').string\n",
    "    return int(results.split()[-2].replace(',',''))\n",
    "\n",
    "def getResultsPerPage(url=None,bs=None):\n",
    "\n",
    "    assert url or bs, 'no inputs'\n",
    "    if url and not bs:\n",
    "        html = requests.get(url + '&_pgn=1').text\n",
    "        bs = BeautifulSoup(html,'html.parser')\n",
    "        \n",
    "    return len(bs.body.find(class_='container').find_all(class_='s-item isKebab'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getEbayData(url):\n",
    "    \n",
    "    html = requests.get(url + '&_pgn=1').text\n",
    "    bs = BeautifulSoup(html,'html.parser')\n",
    "    \n",
    "    nResults = getResults(bs=bs)\n",
    "    resultsPerPage = getResultsPerPage(bs=bs)\n",
    "    npages = ceil(nResults/resultsPerPage)\n",
    "    data = []\n",
    "\n",
    "    for page in range(1,npages):\n",
    "        html = requests.get(url + '&_pgn=' + str(page)).text\n",
    "        sub = BeautifulSoup(html,'html.parser')\n",
    "        info = sub.body.find(class_='container')\n",
    "\n",
    "        search = sub.title.string\n",
    "\n",
    "        for item in info.find_all(class_='s-item isKebab'):\n",
    "            dic = {}\n",
    "        \n",
    "        \n",
    "            price = item.find(class_='s-item__price')\n",
    "            if price:\n",
    "                dic['price'] = float(price.string[3:])\n",
    "            \n",
    "            \n",
    "            title = item.find(class_=\"s-item__title\").string\n",
    "            if title:\n",
    "                dic['title'] = title\n",
    "            else:\n",
    "                title = item.find(class_=\"LIGHT_HIGHLIGHT\")\n",
    "                if title:\n",
    "                    dic['title'] = title.next_sibling.string\n",
    "                else:\n",
    "                    dic['title'] = item.find(class_=\"s-item__title s-item__title--has-tags\").find('div').next_sibling.string\n",
    "\n",
    "                \n",
    "            shipping = item.find(class_='s-item__shipping s-item__logisticsCost').string[3:-9]\n",
    "            if shipping == 'e': \n",
    "                dic['shipping'] = 0\n",
    "            else: \n",
    "                dic['shipping'] = float(shipping)\n",
    "\n",
    "                \n",
    "            time_item = item.find(class_='s-item__time')\n",
    "            if time_item:\n",
    "                bid_time = time_item.find(class_='clipped').string\n",
    "                \n",
    "                try: bid_time = datetime.strptime(bid_time,'Ending %d %b at %H:%M EDT')\n",
    "                except: pass\n",
    "                try: bid_time = datetime.strptime(bid_time,'%d %b at %H:%M')\n",
    "                except: pass\n",
    "                try: bid_time = datetime.strptime(bid_time,'Ending %d %b at %H:%M EST')\n",
    "                except: pass\n",
    "                \n",
    "                assert type(bid_time) != str, bid_time #i give up give me a hint\n",
    "                \n",
    "                time_left = item.find(class_='s-item__time').find(class_='s-item__time-left').string #difficult to parse\n",
    "\n",
    "                bid_time = bid_time.replace(year=datetime.now().year)  \n",
    "                dic['end'] = bid_time\n",
    "\n",
    "                \n",
    "            dic['url'] = item.find(class_='s-item__link')['href']\n",
    "            sURL = sellerURL(dic['url'])\n",
    "            dic['seller_items'] = getResults(url=sURL)\n",
    "\n",
    "            data.append(dic)\n",
    "\n",
    "    df = pd.DataFrame(data,columns=['title','price','shipping','end','seller_items','url'])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.ebay.ca/b/NVIDIA-GeForce-GTX-1070-Graphics-Video-Cards/27386/bn_110679507?rt=nc&LH_Auction=1&_ipg=200&LH_Complete=1'\n",
    "filename = 'gtx1070 history.csv'\n",
    "df = getEbayData(url)\n",
    "df.to_csv(filename, sep=',')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
